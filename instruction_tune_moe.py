"""
MoEæ¨¡å‹æŒ‡ä»¤å¾®è°ƒè„šæœ¬
æ”¯æŒå¤šç§æŒ‡ä»¤å¾®è°ƒæ ¼å¼å’Œè®­ç»ƒç­–ç•¥
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, get_linear_schedule_with_warmup
import json
import os
from tqdm import tqdm
import wandb
from typing import Dict, List, Optional, Tuple
import numpy as np
import random
from datasets import load_dataset
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from enhanced_moe_model import create_moe_model, ChineseEcommerceMoE, load_tokenizer


class InstructionDataset(Dataset):
    """æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†"""
    def __init__(self, instructions: List[Dict], tokenizer, max_length: int = 1024):
        self.instructions = instructions
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.instructions)
    
    def __getitem__(self, idx):
        item = self.instructions[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        if 'system' in item and item['system']:
            input_text = f"<|system|>\n{item['system']}\n<|user|>\n{item['instruction']}\n<|assistant|>\n"
        else:
            input_text = f"<|user|>\n{item['instruction']}\n<|assistant|>\n"
        
        # æ„å»ºå®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å«è¾“å‡ºï¼‰
        full_text = input_text + item['output'] + self.tokenizer.eos_token
        
        # åˆ†è¯
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        
        # åˆ›å»ºæ ‡ç­¾ - åªé¢„æµ‹assistantéƒ¨åˆ†
        labels = input_ids.clone()
        
        # æ‰¾åˆ°assistantå¼€å§‹çš„ä½ç½®
        assistant_token = self.tokenizer.convert_tokens_to_ids('<|assistant|>')
        assistant_start = torch.where(input_ids == assistant_token)[0]
        if len(assistant_start) > 0:
            assistant_start = assistant_start[0] + 1  # è·³è¿‡assistant token
            labels[:assistant_start] = -100  # ä¸è®¡ç®—å‰é¢çš„æŸå¤±
        
        # å¿½ç•¥paddingéƒ¨åˆ†
        labels[attention_mask == 0] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


class InstructionDataManager:
    """æŒ‡ä»¤æ•°æ®ç®¡ç†å™¨"""
    def __init__(self, tokenizer, max_length: int = 1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def load_alpaca_data(self, limit: int = 50000):
        """åŠ è½½Alpacaæ ¼å¼æ•°æ®"""
        try:
            dataset = load_dataset('tatsu-lab/alpaca', split='train')
            instructions = []
            for i, item in enumerate(dataset):
                if i >= limit:
                    break
                instructions.append({
                    'instruction': item['instruction'],
                    'input': item['input'],
                    'output': item['output'],
                    'system': ''
                })
            return instructions
        except Exception as e:
            logger.warning(f"Failed to load Alpaca data: {e}")
            return []
    
    def load_belle_data(self, limit: int = 50000):
        """åŠ è½½BELLEä¸­æ–‡æ•°æ®"""
        try:
            dataset = load_dataset('BelleGroup/train_0.5M_CN', split='train', streaming=True)
            instructions = []
            for i, item in enumerate(dataset):
                if i >= limit:
                    break
                instructions.append({
                    'instruction': item['instruction'],
                    'input': item['input'],
                    'output': item['output'],
                    'system': 'ä½ æ˜¯ä¸€ä¸ª helpful AI assistantã€‚'
                })
            return instructions
        except Exception as e:
            logger.warning(f"Failed to load BELLE data: {e}")
            return []
    
    def load_ecommerce_instructions(self):
        """åŠ è½½ç”µå•†é¢†åŸŸæŒ‡ä»¤æ•°æ®"""
        return [
            {
                'instruction': 'è¯·ä¸ºè¿™æ¬¾æ–°æ‰‹æœºå†™ä¸€æ®µäº§å“æè¿°',
                'input': '',
                'output': 'è¿™æ¬¾æ™ºèƒ½æ‰‹æœºé‡‡ç”¨6.7è‹±å¯¸OLEDæ˜¾ç¤ºå±ï¼Œé…å¤‡é«˜é€šéªé¾™8å¤„ç†å™¨ï¼Œå†…å­˜12GB+256GBï¼Œæ”¯æŒ5Gç½‘ç»œï¼Œæ‹ç…§æ•ˆæœå‡ºè‰²ï¼Œç»­èˆªèƒ½åŠ›å¼ºï¼Œæ˜¯æ‚¨çš„ç†æƒ³é€‰æ‹©ã€‚',
                'system': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†äº§å“æè¿°æ’°å†™ä¸“å®¶ã€‚'
            },
            {
                'instruction': 'è¯·å›å¤è¿™ä¸ªå®¢æˆ·çš„å’¨è¯¢',
                'input': 'å®¢æˆ·é—®ï¼šè¿™æ¬¾æ‰‹æœºæ”¯æŒå¿«å……å—ï¼Ÿ',
                'output': 'æ‚¨å¥½ï¼è¿™æ¬¾æ‰‹æœºæ”¯æŒ65Wè¶…çº§å¿«å……ï¼Œ30åˆ†é’Ÿå¯ä»¥å……ç”µåˆ°80%ï¼Œå®Œå…¨æ»¡è¶³æ‚¨çš„æ—¥å¸¸ä½¿ç”¨éœ€æ±‚ã€‚åŒæ—¶æˆ‘ä»¬è¿˜æä¾›ä¸€å¹´è´¨ä¿æœåŠ¡ï¼Œè¯·æ‚¨æ”¾å¿ƒè´­ä¹°ã€‚',
                'system': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†å®¢æœã€‚'
            },
            {
                'instruction': 'è¯·ä¸ºåŒåä¸€æ´»åŠ¨å†™ä¸€æ®µè¥é”€æ–‡æ¡ˆ',
                'input': '',
                'output': 'ğŸ‰åŒåä¸€ç‹‚æ¬¢èŠ‚æ¥å•¦ï¼å…¨åœºå•†å“5æŠ˜èµ·ï¼Œæ»¡299å‡50ï¼Œæ»¡599å‡120ï¼é™æ—¶ç§’æ€æ¯æ—¥0ç‚¹å¼€å¯ï¼ŒiPhone 15ä»…éœ€4999å…ƒï¼è¿˜æœ‰æœºä¼šèµ¢å–å…å•å¤§å¥–ï¼Œå¿«æ¥æŠ¢è´­å§ï¼ğŸ›’',
                'system': 'ä½ æ˜¯ä¸€ä¸ªåˆ›æ„è¥é”€æ–‡æ¡ˆä¸“å®¶ã€‚'
            },
            {
                'instruction': 'è¯·åˆ†æè¿™ä¸ªå•†å“è¯„ä»·çš„æƒ…æ„Ÿ',
                'input': 'å•†å“æ”¶åˆ°äº†ï¼Œè´¨é‡ä¸€èˆ¬èˆ¬ï¼ŒåŒ…è£…ä¹Ÿå¾ˆç®€é™‹ï¼Œæ„Ÿè§‰ä¸å€¼è¿™ä¸ªä»·æ ¼ã€‚',
                'output': 'è¿™æ¡è¯„ä»·çš„æƒ…æ„Ÿå€¾å‘æ˜¯è´Ÿé¢çš„ã€‚è¯„ä»·è€…è¡¨è¾¾äº†å¯¹äº§å“è´¨é‡å’ŒåŒ…è£…çš„ä¸æ»¡ï¼Œè®¤ä¸ºæ€§ä»·æ¯”ä¸é«˜ã€‚å»ºè®®å…³æ³¨äº§å“è´¨é‡æ§åˆ¶å’ŒåŒ…è£…æ”¹è¿›ã€‚',
                'system': 'ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚'
            }
        ]
    
    def create_instruction_datasets(self, batch_size: int = 8) -> Dict[str, DataLoader]:
        """åˆ›å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†"""
        logger.info("Loading instruction tuning data...")
        
        # åŠ è½½å„ç±»æŒ‡ä»¤æ•°æ®
        alpaca_data = self.load_alpaca_data(limit=10000)
        belle_data = self.load_belle_data(limit=10000)
        ecommerce_data = self.load_ecommerce_instructions()
        
        # åˆå¹¶æ•°æ®
        all_instructions = alpaca_data + belle_data + ecommerce_data
        np.random.shuffle(all_instructions)
        
        # åˆ†å‰²æ•°æ®é›†
        total_size = len(all_instructions)
        train_size = int(0.8 * total_size)
        val_size = int(0.1 * total_size)
        
        train_instructions = all_instructions[:train_size]
        val_instructions = all_instructions[train_size:train_size + val_size]
        test_instructions = all_instructions[train_size + val_size:]
        
        logger.info(f"Dataset sizes - Train: {len(train_instructions)}, Val: {len(val_instructions)}, Test: {len(test_instructions)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        datasets = {}
        for split, instructions in [('train', train_instructions), ('val', val_instructions), ('test', test_instructions)]:
            dataset = InstructionDataset(instructions, self.tokenizer, self.max_length)
            shuffle = (split == 'train')
            datasets[split] = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=2,
                pin_memory=True
            )
        
        return datasets


class InstructionTuner:
    """æŒ‡ä»¤å¾®è°ƒå™¨"""
    def __init__(self, model: ChineseEcommerceMoE, tokenizer,
                 learning_rate: float = 2e-5, num_epochs: int = 5,
                 warmup_steps: int = 100, save_dir: str = "./instruction_checkpoints",
                 use_wandb: bool = True):
        
        self.model = model
        self.tokenizer = tokenizer
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.warmup_steps = warmup_steps
        self.save_dir = save_dir
        self.use_wandb = use_wandb
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.999),
            weight_decay=0.01
        )
        
        # ä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_eval_loss = float('inf')
        
        # åˆå§‹åŒ–wandb
        if use_wandb:
            try:
                wandb.init(
                    project="chinese-ecommerce-moe-instruction",
                    config={
                        "model_size": "0.6B",
                        "learning_rate": learning_rate,
                        "num_epochs": num_epochs,
                        "warmup_steps": warmup_steps,
                        "total_params": model.num_parameters,
                        "active_params": model.num_active_parameters
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to initialize wandb: {e}")
                self.use_wandb = False
    
    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_lm_loss = 0
        total_aux_loss = 0
        num_steps = len(dataloader)
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}/{self.num_epochs}")
        
        for step, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                return_dict=True
            )
            
            loss = outputs['loss']
            lm_loss = loss - outputs['aux_loss']  # è¿‘ä¼¼è®¡ç®—
            aux_loss = outputs['aux_loss']
            
            # è®°å½•æŸå¤±
            total_loss += loss.item()
            total_lm_loss += lm_loss.item()
            total_aux_loss += aux_loss.item()
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            self.global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lm_loss': f'{lm_loss.item():.4f}',
                'aux_loss': f'{aux_loss.item():.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # è®°å½•åˆ°wandb
            if self.use_wandb and self.global_step % 50 == 0:
                wandb.log({
                    'train_loss': loss.item(),
                    'lm_loss': lm_loss.item(),
                    'aux_loss': aux_loss.item(),
                    'learning_rate': self.optimizer.param_groups[0]['lr'],
                    'global_step': self.global_step
                })
        
        return {
            'train_loss': total_loss / num_steps,
            'lm_loss': total_lm_loss / num_steps,
            'aux_loss': total_aux_loss / num_steps
        }
    
    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        total_lm_loss = 0
        total_aux_loss = 0
        num_steps = len(dataloader)
        
        with torch.no_grad():
            progress_bar = tqdm(dataloader, desc="Evaluating")
            
            for batch in progress_bar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels,
                    return_dict=True
                )
                
                loss = outputs['loss']
                lm_loss = loss - outputs['aux_loss']
                aux_loss = outputs['aux_loss']
                
                total_loss += loss.item()
                total_lm_loss += lm_loss.item()
                total_aux_loss += aux_loss.item()
                
                progress_bar.set_postfix({
                    'eval_loss': f'{loss.item():.4f}'
                })
        
        return {
            'eval_loss': total_loss / num_steps,
            'lm_loss': total_lm_loss / num_steps,
            'aux_loss': total_aux_loss / num_steps
        }
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'config': self.model.config.__dict__
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        save_path = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pt')
        torch.save(checkpoint, save_path)
        logger.info(f"Checkpoint saved: {save_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_model.pt')
            torch.save(checkpoint, best_path)
            logger.info(f"Best model saved: {best_path}")
        
        # ä¿å­˜ä¸ºHuggingFaceæ ¼å¼
        hf_save_path = os.path.join(self.save_dir, f'hf_model_epoch_{epoch}')
        os.makedirs(hf_save_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(self.model.state_dict(), os.path.join(hf_save_path, 'pytorch_model.bin'))
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(hf_save_path, 'config.json'), 'w') as f:
            json.dump(self.model.config.__dict__, f, indent=2)
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(hf_save_path)
        logger.info(f"HuggingFace model saved: {hf_save_path}")
    
    def train(self, train_dataloader: DataLoader, val_dataloader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info(f"Starting instruction tuning on {self.device}")
        logger.info(f"Model parameters: {self.model.num_parameters:,}")
        logger.info(f"Active parameters: {self.model.num_active_parameters:,}")
        
        for epoch in range(1, self.num_epochs + 1):
            logger.info(f"\nEpoch {epoch}/{self.num_epochs}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_dataloader, epoch)
            
            # è¯„ä¼°
            eval_metrics = self.evaluate(val_dataloader)
            
            # è®°å½•ç»“æœ
            logger.info(f"Train Loss: {train_metrics['train_loss']:.4f}")
            logger.info(f"Eval Loss: {eval_metrics['eval_loss']:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = eval_metrics['eval_loss'] < self.best_eval_loss
            if is_best:
                self.best_eval_loss = eval_metrics['eval_loss']
            
            self.save_checkpoint(epoch, {**train_metrics, **eval_metrics}, is_best)
            
            # è®°å½•åˆ°wandb
            if self.use_wandb:
                wandb.log({
                    'epoch': epoch,
                    **train_metrics,
                    **eval_metrics
                })
        
        logger.info("Instruction tuning completed!")
        logger.info(f"Best eval loss: {self.best_eval_loss:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    config = {
        'model_size': '0.6B',
        'batch_size': 8,
        'learning_rate': 2e-5,
        'num_epochs': 5,
        'warmup_steps': 100,
        'max_length': 1024,
        'save_dir': './instruction_checkpoints'
    }
    
    # åŠ è½½tokenizer
    logger.info("Loading tokenizer...")
    tokenizer = load_tokenizer()
    
    # æ·»åŠ ç‰¹æ®Štoken
    special_tokens = {
        'additional_special_tokens': ['<|system|>', '<|user|>', '<|assistant|>']
    }
    tokenizer.add_special_tokens(special_tokens)
    
    # åˆ›å»ºæ¨¡å‹
    logger.info(f"Creating {config['model_size']} model...")
    model = create_moe_model(config['model_size'])
    model.resize_token_embeddings(len(tokenizer))
    
    # æ•°æ®ç®¡ç†å™¨
    logger.info("Preparing instruction data...")
    data_manager = InstructionDataManager(tokenizer, config['max_length'])
    dataloaders = data_manager.create_instruction_datasets(config['batch_size'])
    
    # è®­ç»ƒå™¨
    trainer = InstructionTuner(
        model=model,
        tokenizer=tokenizer,
        learning_rate=config['learning_rate'],
        num_epochs=config['num_epochs'],
        warmup_steps=config['warmup_steps'],
        save_dir=config['save_dir']
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(dataloaders['train'], dataloaders['val'])
    
    logger.info("Instruction tuning completed successfully!")


if __name__ == "__main__":
    main()